{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'project1-2023' already exists and is not an empty directory.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cr-nlp/project1-2023.git\n",
    "\n",
    "import urllib.request as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def loadNFCorpus():\n",
    "\tdir = \"./project1-2023/\"\n",
    "\tfilename = dir +\"dev.docs\"\n",
    "\n",
    "\tdicDoc={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\t#print(tabLine)\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\t#print(value)\n",
    "\t\tdicDoc[key] = value\n",
    "\tfilename = dir + \"dev.all.queries\"\n",
    "\tdicReq={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\tdicReq[key] = value\n",
    "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
    "\tdicReqDoc=defaultdict(dict)\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.strip().split('\\t')\n",
    "\t\treq = tabLine[0]\n",
    "\t\tdoc = tabLine[2]\n",
    "\t\tscore = int(tabLine[3])\n",
    "\t\tdicReqDoc[req][doc]=score\n",
    "\n",
    "\treturn dicDoc, dicReq, dicReqDoc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2TokenList(text):\n",
    "\tstopword = stopwords.words('english')\n",
    "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
    "\tword_tokens = word_tokenize(text.lower())\n",
    "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
    "\treturn word_tokens_without_stops\n",
    "\n",
    "def run_bm25_only(startDoc,endDoc):\n",
    "\n",
    "\tdicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
    "\n",
    "\tdocsToKeep=[]\n",
    "\treqsToKeep=[]\n",
    "\tdicReqDocToKeep=defaultdict(dict)\n",
    "\n",
    "\t#150\n",
    "\tndcgTop=10\n",
    "\t#print(\"ndcgTop=\",ndcgTop,\"nbDocsToKeep=\",nbDocsToKeep)\n",
    "\n",
    "\ti=startDoc\n",
    "\tfor reqId in dicReqDoc:\n",
    "\t\tif i > (endDoc - startDoc) :  #nbDocsToKeep:\n",
    "\t\t\tbreak\n",
    "\t\tfor docId in dicReqDoc[reqId]:\n",
    "\t\t\tdicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
    "\t\t\tdocsToKeep.append(docId)\n",
    "\t\t\ti = i + 1\n",
    "\t\treqsToKeep.append(reqId)\n",
    "\tdocsToKeep = list(set(docsToKeep))\n",
    "\n",
    "\t#\"\"\"\n",
    "\tallVocab ={}\n",
    "\tfor k in docsToKeep:\n",
    "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
    "\t\t#print(docTokenList)\n",
    "\t\tfor word in docTokenList:\n",
    "\t\t\tif word not in allVocab:\n",
    "\t\t\t\tallVocab[word] = word\n",
    "\tallVocabListDoc = list(allVocab)\n",
    "\t#print(\"doc vocab=\",len(allVocabListDoc))\n",
    "\tallVocab ={}\n",
    "\tfor k in reqsToKeep:\n",
    "\t\tdocTokenList = text2TokenList(dicReq[k])\n",
    "\t\t#print(docTokenList)\n",
    "\t\tfor word in docTokenList:\n",
    "\t\t\tif word not in allVocab:\n",
    "\t\t\t\tallVocab[word] = word\n",
    "\tallVocabListReq = list(allVocab)\n",
    "\n",
    "\tfrom rank_bm25 import BM25Okapi\n",
    "\n",
    "\tcorpusDocTokenList = []\n",
    "\tcorpusReqTokenList = {}\n",
    "\tcorpusDocName=[]\n",
    "\tcorpusDicoDocName={}\n",
    "\ti = 0\n",
    "\tfor k in docsToKeep:\n",
    "\t\tdocTokenList = text2TokenList(dicDoc[k])\n",
    "\t\tcorpusDocTokenList.append(docTokenList)\n",
    "\t\tcorpusDocName.append(k)\n",
    "\t\tcorpusDicoDocName[k] = i\n",
    "\t\ti = i + 1\n",
    "\n",
    "\t#print(\"reqs...\")\n",
    "\tcorpusReqName=[]\n",
    "\tcorpusDicoReqName={}\n",
    "\ti = 0\n",
    "\tfor k in reqsToKeep:\n",
    "\t\treqTokenList = text2TokenList(dicReq[k])\n",
    "\t\tcorpusReqTokenList[k] = reqTokenList\n",
    "\t\tcorpusReqName.append(k)\n",
    "\t\tcorpusDicoReqName[k] = i\n",
    "\t\ti = i + 1\n",
    "\n",
    "\t#print(\"bm25 doc indexing...\")\n",
    "\tbm25 = BM25Okapi(corpusDocTokenList)\n",
    "\n",
    "\tndcgCumul=0\n",
    "\tcorpusReqVec={}\n",
    "\tndcgBM25Cumul=0\n",
    "\tnbReq=0\n",
    "\n",
    "\tfrom sklearn.metrics import ndcg_score\n",
    "\tfor req in corpusReqTokenList:\n",
    "\t\tj=0\n",
    "\t\treqTokenList = corpusReqTokenList[req]\n",
    "\t\tdoc_scores = bm25.get_scores(reqTokenList)\n",
    "\t\ttrueDocs = np.zeros(len(corpusDocTokenList))\n",
    "\n",
    "\t\tfor docId in corpusDicoDocName:\n",
    "\t\t\tif req in dicReqDocToKeep:\n",
    "\t\t\t\tif docId in dicReqDocToKeep[req]:\n",
    "\t\t\t\t\t#get position docId\n",
    "\t\t\t\t\tposDocId = corpusDicoDocName[docId]\n",
    "\t\t\t\t\ttrueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
    "\t\t\t\t\t#print(\"TOKEEP=\",docId)\n",
    "\t\t\t\t\t#print(trueDocs)\n",
    "\t\tndcgBM25Cumul = ndcgBM25Cumul + ndcg_score([trueDocs], [doc_scores],k=ndcgTop)\n",
    "\t\tnbReq = nbReq + 1\n",
    "\tndcgBM25Cumul = ndcgBM25Cumul / nbReq\n",
    "\tprint(\"ndcg bm25=\",ndcgBM25Cumul)\n",
    "\treturn ndcgBM25Cumul\n",
    "\n",
    "nb_docs = 3192 #all docs\n",
    "#nb_docs = 150 #for tests\n",
    "run_bm25_only(0,nb_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.3 GiB for an array with shape (16545452, 200) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f893f632d852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load BioWordVec embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbio_word_vec_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/alexa/Downloads/BioWordVec_PubMed_MIMICIII_d200.vec.bin'\u001b[0m  \u001b[1;31m# Update with your file path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbio_word_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbio_word_vec_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1723\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1724\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1725\u001b[1;33m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1726\u001b[0m         )\n\u001b[0;32m   1727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2064\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m         \u001b[0mkv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vector_size, count, dtype, mapfile_path)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# formerly known as syn0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.3 GiB for an array with shape (16545452, 200) and data type float32"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load BioWordVec embeddings\n",
    "bio_word_vec_path = 'C:/Users/alexa/Downloads/BioWordVec_PubMed_MIMICIII_d200.vec.bin'  # Update with your file path\n",
    "bio_word_vec = KeyedVectors.load_word2vec_format(bio_word_vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "def averaged_doc_representation(corpus, model):\n",
    "    vocabulary = set(model.index_to_key)\n",
    "    num_features = model.vector_size\n",
    "    doc_feature_vectors = [average_word_vectors(doc, model, vocabulary, num_features) for doc in corpus]\n",
    "    return doc_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text2TokenList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ed209460ff2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mnb_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3192\u001b[0m  \u001b[1;31m# Use the full range or adjust as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mrun_with_word2vec_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-ed209460ff2a>\u001b[0m in \u001b[0;36mrun_with_word2vec_embeddings\u001b[1;34m(startDoc, endDoc)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Prepare the document vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdoc_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maveraged_doc_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext2TokenList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicDoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocsToKeep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbio_word_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mndcgCumul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ed209460ff2a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Prepare the document vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdoc_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maveraged_doc_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext2TokenList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicDoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocsToKeep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbio_word_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mndcgCumul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text2TokenList' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming functions like loadNFCorpus, text2TokenList exist as in your original code\n",
    "\n",
    "\n",
    "\n",
    "def rank_documents(query_vector, doc_vectors):\n",
    "    similarities = cosine_similarity([query_vector], doc_vectors)\n",
    "    return similarities[0]\n",
    "\n",
    "def run_with_word2vec_embeddings(startDoc, endDoc):\n",
    "    dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
    "\n",
    "    docsToKeep, reqsToKeep, dicReqDocToKeep = select_documents(dicReqDoc, startDoc, endDoc)\n",
    "    \n",
    "    # Prepare the document vectors\n",
    "    doc_vectors = averaged_doc_representation([text2TokenList(dicDoc[doc]) for doc in docsToKeep], bio_word_vec)\n",
    "\n",
    "    ndcgCumul = 0\n",
    "    nbReq = 0\n",
    "    ndcgTop = 10  # Set the top N for NDCG calculation\n",
    "\n",
    "    for req in reqsToKeep:\n",
    "        reqTokenList = text2TokenList(dicReq[req])\n",
    "        query_vector = average_word_vectors(reqTokenList, bio_word_vec, set(bio_word_vec.index_to_key), bio_word_vec.vector_size)\n",
    "        trueRelevance = get_true_relevance(req, dicReqDocToKeep, docsToKeep)\n",
    "\n",
    "        predicted_scores = rank_documents(query_vector, doc_vectors)\n",
    "        ndcg_value = ndcg_score([trueRelevance], [predicted_scores], k=ndcgTop)\n",
    "\n",
    "        ndcgCumul += ndcg_value\n",
    "        nbReq += 1\n",
    "\n",
    "    average_ndcg = ndcgCumul / nbReq\n",
    "    print(\"Average NDCG with Word2Vec Embeddings:\", average_ndcg)\n",
    "    return average_ndcg\n",
    "\n",
    "def get_true_relevance(req, dicReqDocToKeep, docsToKeep):\n",
    "    true_relevance = [0] * len(docsToKeep)\n",
    "    if req in dicReqDocToKeep:\n",
    "        for docId, relevance in dicReqDocToKeep[req].items():\n",
    "            if docId in docsToKeep:\n",
    "                index = docsToKeep.index(docId)\n",
    "                true_relevance[index] = relevance\n",
    "    return true_relevance\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nb_docs = 3192  # Use the full range or adjust as needed\n",
    "run_with_word2vec_embeddings(0, nb_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\Anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "fatal: destination path 'project1-2023' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import urllib.request as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import pipeline\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "\n",
    "\tstopword = stopwords.words('english')\n",
    "\n",
    "\t# Remove URLs\n",
    "\tsentence = re.sub(r'http\\S+', '', sentence, flags=re.MULTILINE)\n",
    "\tsentence = re.sub(r'www\\S+', '', sentence, flags=re.MULTILINE)\n",
    "\n",
    "\t# Remove all numbers\n",
    "\tsentence = re.sub(r'\\d', '', sentence)\n",
    "\n",
    "\t# Remove punctuation except for certain cases\n",
    "\tsentence = re.sub(r'[^\\w\\s\\-/]', '', sentence)\n",
    "\n",
    "\tword_tokens = word_tokenize(sentence.lower())\n",
    "\t\t\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tnormalised_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]\n",
    "\n",
    "\n",
    "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>3]\n",
    "\treturn word_tokens_without_stops\n",
    "\n",
    "\n",
    "# we improved this by using a set\n",
    "def generate_vocab(kept, loaded):\n",
    "\n",
    "  vocab = set()\n",
    "\n",
    "  for k in kept:\n",
    "    vocab.update(clean_sentence(loaded[k]))\n",
    "\n",
    "  return list(vocab)\n",
    "\n",
    "\n",
    "# improved version with i and k by using enumerate by index tracking, this avoids using i += 1.\n",
    "def generate_corpus(kept, loaded, corpus_token_list):\n",
    "    corpus_name = []\n",
    "    corpus_dic = {}\n",
    "\n",
    "    for i, k in enumerate(kept):\n",
    "        token_list = clean_sentence(loaded[k])\n",
    "\n",
    "        if isinstance(corpus_token_list, dict):\n",
    "          corpus_token_list[k] = (token_list)\n",
    "        else:\n",
    "          corpus_token_list.append(token_list)\n",
    "\n",
    "        corpus_name.append(k)\n",
    "        corpus_dic[k] = i\n",
    "\n",
    "    return corpus_token_list, corpus_name, corpus_dic\n",
    "\n",
    "def summarise_corpus(kept, loaded):\n",
    "    summariser = pipeline('summarization')\n",
    "    summaries = []\n",
    "\n",
    "    for doc in kept:\n",
    "        summaries.append(summariser(loaded[doc], min_length=10, max_length = 100, do_sample=False))\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def normalise(score):\n",
    "    return (score - np.mean(score)) / np.std(score)\n",
    "\n",
    "# keep relevant: only keeps the relevant values together\n",
    "def shorten_dictionary(doc, req, req_doc, keep_relevant, start, end):\n",
    "  kept_doc = []\n",
    "  kept_req = []\n",
    "  kept_req_doc = defaultdict(dict)\n",
    "\n",
    "  if(keep_relevant):\n",
    "    req_doc = {outer_key: {inner_key: inner_value for inner_key, inner_value in inner_dict.items() if inner_value == 2}\n",
    "                            for outer_key, inner_dict in req_doc.items()}\n",
    "\n",
    "  count = start\n",
    "  for req in req_doc:\n",
    "    if count > (end - start):\n",
    "      break\n",
    "    for doc in req_doc[req]:\n",
    "      kept_req_doc[req][doc] = req_doc[req][doc]\n",
    "      kept_doc.append(doc)\n",
    "      count += 1\n",
    "    kept_req.append(req)\n",
    "  kept_doc = list(set(kept_doc)) # we remove duplicates\n",
    "\n",
    "  return kept_doc, kept_req, kept_req_doc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming functions like loadNFCorpus, text2TokenList exist as in your original code\n",
    "# Load your BioWordVec embeddings\n",
    "bio_word_vec_path = 'C:/Users/alexa/Downloads/BioWordVec_PubMed_MIMICIII_d200.vec.bin'  # Update with your file path\n",
    "bio_word_vec = KeyedVectors.load_word2vec_format(bio_word_vec_path, binary=True)\n",
    "\n",
    "!git clone https://github.com/cr-nlp/project1-2023.git\n",
    "\n",
    "\n",
    "def loadNFCorpus():\n",
    "\tdir = \"./project1-2023/\"\n",
    "\tfilename = dir +\"dev.docs\"\n",
    "\n",
    "\tdicDoc={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\t#print(tabLine)\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\t#print(value)\n",
    "\t\tdicDoc[key] = value\n",
    "\tfilename = dir + \"dev.all.queries\"\n",
    "\tdicReq={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\tdicReq[key] = value\n",
    "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
    "\tdicReqDoc=defaultdict(dict)\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.strip().split('\\t')\n",
    "\t\treq = tabLine[0]\n",
    "\t\tdoc = tabLine[2]\n",
    "\t\tscore = int(tabLine[3])\n",
    "\t\tdicReqDoc[req][doc]=score\n",
    "\n",
    "\treturn dicDoc, dicReq, dicReqDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG with Hybrid Method: 0.7595836394196158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7595836394196158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def normalize_scores(scores):\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    if std_score != 0:\n",
    "        normalized_scores = (scores - mean_score) / std_score\n",
    "    else:\n",
    "        normalized_scores = scores - mean_score  # Avoid division by zero\n",
    "    return normalized_scores\n",
    "\n",
    "\n",
    "def select_documents(dicReqDoc, startDoc, endDoc):\n",
    "    docsToKeep = []\n",
    "    reqsToKeep = []\n",
    "    dicReqDocToKeep = defaultdict(dict)\n",
    "\n",
    "    i = 0\n",
    "    for reqId, docs in dicReqDoc.items():\n",
    "        if i > endDoc - startDoc:\n",
    "            break\n",
    "        for docId in docs:\n",
    "            dicReqDocToKeep[reqId][docId] = dicReqDoc[reqId][docId]\n",
    "            docsToKeep.append(docId)\n",
    "            i += 1\n",
    "        reqsToKeep.append(reqId)\n",
    "    docsToKeep = list(set(docsToKeep))\n",
    "\n",
    "    return docsToKeep, reqsToKeep, dicReqDocToKeep\n",
    "# Function to compute average word vectors\n",
    "\n",
    "def text2TokenList(text):\n",
    "\tstopword = stopwords.words('english')\n",
    "\t#print(\"LEN DE STOPWORD=\",len(stopword))\n",
    "\tword_tokens = word_tokenize(text.lower())\n",
    "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word)>2]\n",
    "\treturn word_tokens_without_stops\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "# Function to get averaged document representation\n",
    "def averaged_doc_representation(corpus, model):\n",
    "    vocabulary = set(model.index_to_key)\n",
    "    num_features = model.vector_size\n",
    "    doc_feature_vectors = [average_word_vectors(doc, model, vocabulary, num_features) for doc in corpus]\n",
    "    return doc_feature_vectors\n",
    "\n",
    "# Function to compute BM25 scores\n",
    "def compute_bm25_scores(corpus, query):\n",
    "    tokenized_corpus = [text2TokenList(doc) for doc in corpus]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    query_tokens = clean_sentence(query)\n",
    "\n",
    "    #query_tokens = text2TokenList(query)\n",
    "    bm25_scores = bm25.get_scores(query_tokens)\n",
    "    normalized_bm25_scores = normalize_scores(np.array(bm25_scores))\n",
    "    return normalized_bm25_scores\n",
    "\n",
    "# Function to compute cosine similarity scores\n",
    "def compute_cosine_scores(docs, query, bio_word_vec):\n",
    "    query_vector = average_word_vectors(text2TokenList(query), bio_word_vec, set(bio_word_vec.index_to_key), bio_word_vec.vector_size)\n",
    "    doc_vectors = averaged_doc_representation([text2TokenList(doc) for doc in docs], bio_word_vec)\n",
    "    cosine_scores = rank_documents(query_vector, doc_vectors)\n",
    "    normalized_cosine_scores = normalize_scores(np.array(cosine_scores))\n",
    "    return normalized_cosine_scores\n",
    "\n",
    "# Function to compute hybrid score\n",
    "def hybrid_score(bm25_scores, cosine_scores, alpha=0.7):\n",
    "    combined_scores = alpha * bm25_scores + (1 - alpha) * cosine_scores\n",
    "    return combined_scores\n",
    "\n",
    "# Function to rank documents\n",
    "def rank_documents(query_vector, doc_vectors):\n",
    "    similarities = cosine_similarity([query_vector], doc_vectors)\n",
    "    return similarities[0]\n",
    "\n",
    "# Function to evaluate with NDCG\n",
    "def evaluate_with_ndcg(reqs, docs, dicDoc,dicReq, dicReqDocToKeep, bm25, bio_word_vec, alpha=0.5, ndcgTop=5):\n",
    "    ndcgCumul = 0\n",
    "    nbReq = 0\n",
    "    \n",
    "    for req in reqs:\n",
    "        req_text = dicReq[req]\n",
    "        bm25_scores = compute_bm25_scores([dicDoc[doc] for doc in docs], req_text)\n",
    "        cosine_scores = compute_cosine_scores([dicDoc[doc] for doc in docs], req_text, bio_word_vec)\n",
    "        hybrid_scores = hybrid_score(bm25_scores, cosine_scores, alpha)\n",
    "\n",
    "        trueRelevance = get_true_relevance(req, dicReqDocToKeep, docs)\n",
    "        ndcg_value = ndcg_score([trueRelevance], [hybrid_scores], k=ndcgTop)\n",
    "        ndcgCumul += ndcg_value\n",
    "        nbReq += 1\n",
    "\n",
    "    average_ndcg = ndcgCumul / nbReq\n",
    "    return average_ndcg\n",
    "\n",
    "# Function to get true relevance scores\n",
    "def get_true_relevance(req, dicReqDocToKeep, docs):\n",
    "    true_relevance = [0] * len(docs)\n",
    "    if req in dicReqDocToKeep:\n",
    "        for docId, relevance in dicReqDocToKeep[req].items():\n",
    "            if docId in docs:\n",
    "                index = docs.index(docId)\n",
    "                true_relevance[index] = relevance\n",
    "    return true_relevance\n",
    "\n",
    "# Main function to run\n",
    "def run_with_hybrid_method(startDoc, endDoc, alpha=0.5):\n",
    "    dicDoc, dicReq, dicReqDoc = loadNFCorpus()\n",
    "    docsToKeep, reqsToKeep, dicReqDocToKeep = select_documents(dicReqDoc, startDoc, endDoc)\n",
    "\n",
    "    # Assuming you already have a BM25 model initialized\n",
    "    bm25 = BM25Okapi([text2TokenList(dicDoc[doc]) for doc in docsToKeep])\n",
    "\n",
    "    average_ndcg = evaluate_with_ndcg(reqsToKeep, docsToKeep,dicDoc, dicReq, dicReqDocToKeep, bm25, bio_word_vec, alpha=alpha)\n",
    "    print(\"Average NDCG with Hybrid Method:\", average_ndcg)\n",
    "    return average_ndcg\n",
    "\n",
    "# Example usage\n",
    "nb_docs = 150 #Use the full range or adjust as needed\n",
    "run_with_hybrid_method(0, nb_docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
