{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "## By BROSSEAU Alexandre & COGORDAN Alexandre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the necessary module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import ndcg_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexandrecogordan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexandrecogordan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the NFCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNFCorpus():\n",
    "\tdir = \"./project1-2023/\"\n",
    "\tfilename = dir +\"dev.docs\"\n",
    "\n",
    "\tdicDoc={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\tdicDoc[key] = value\n",
    "\tfilename = dir + \"dev.all.queries\"\n",
    "\tdicReq={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\tdicReq[key] = value\n",
    "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
    "\tdicReqDoc=defaultdict(dict)\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.strip().split('\\t')\n",
    "\t\treq = tabLine[0]\n",
    "\t\tdoc = tabLine[2]\n",
    "\t\tscore = int(tabLine[3])\n",
    "\t\tdicReqDoc[req][doc]=score\n",
    "\n",
    "\treturn dicDoc, dicReq, dicReqDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where we'll change our main parameters to test and evaluate the different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_word_length = 3\n",
    "keep_relevant_documents = False\n",
    "start = 0\n",
    "end = 150\n",
    "w2v_vector_size = 200\n",
    "w2v_window = 10\n",
    "w2v_min_count = 1\n",
    "w2v_sg = 1\n",
    "alpha_value = 0.5\n",
    "chosen_model = 3 # 1 for my model, 2 for glove model, 3 for biovec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a shortened corpus where we only keep the documents that are indicated by start and end\n",
    "# Keep relevant: only keeps the relevant documents to their respective queries\n",
    "\n",
    "def shorten_dictionary(doc, req, req_doc, keep_relevant, start, end):\n",
    "  kept_doc = []\n",
    "  kept_req = []\n",
    "  kept_req_doc = defaultdict(dict)\n",
    "\n",
    "  if(keep_relevant):\n",
    "    req_doc = {outer_key: {inner_key: inner_value for inner_key, inner_value in inner_dict.items() if inner_value == 2}\n",
    "                            for outer_key, inner_dict in req_doc.items()}\n",
    "\n",
    "  count = start\n",
    "  for req in req_doc:\n",
    "    if count > (end - start):\n",
    "      break\n",
    "    for doc in req_doc[req]:\n",
    "      kept_req_doc[req][doc] = req_doc[req][doc]\n",
    "      kept_doc.append(doc)\n",
    "      count += 1\n",
    "    kept_req.append(req)\n",
    "  kept_doc = list(set(kept_doc)) # We remove duplicates\n",
    "\n",
    "  return kept_doc, kept_req, kept_req_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence, minimum_word_length = minimum_word_length):\n",
    "\n",
    "    # Remove stopwords\n",
    "\tstopword = stopwords.words('english')\n",
    "\n",
    "\t# Remove URLs\n",
    "\tsentence = re.sub(r'http\\S+', '', sentence, flags=re.MULTILINE)\n",
    "\tsentence = re.sub(r'www\\S+', '', sentence, flags=re.MULTILINE)\n",
    "\n",
    "\t# Remove numbers\n",
    "\tsentence = re.sub(r'\\d', '', sentence)\n",
    "\n",
    "\t# Remove punctuation except for certain cases (hyphens, apostrophes, etc.)\n",
    "\tsentence = re.sub(r'[^\\w\\s\\-/]', '', sentence)\n",
    "\n",
    "    # We tokenize the sentence\n",
    "\tword_tokens = word_tokenize(sentence.lower())\t\t\n",
    "\t\n",
    "    # We lemmatize the tokens - lemmatization is the process of converting a word to its base form\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tword_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]\n",
    "\n",
    "    # We remove stopwords and words with less than 3 characters\n",
    "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word) > minimum_word_length]\n",
    "\treturn word_tokens_without_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(kept, loaded, summary = False):\n",
    "\n",
    "  vocab = set()\n",
    "\n",
    "  for k in kept:\n",
    "    if(summary != False):\n",
    "      vocab.update(clean_sentence(loaded[k]))\n",
    "    else:\n",
    "      vocab.update(clean_sentence(k))\n",
    "\n",
    "  return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(kept, loaded, corpus_token_list):\n",
    "    corpus_name = []\n",
    "    corpus_dic = {}\n",
    "\n",
    "    for i, k in enumerate(kept):\n",
    "        token_list = clean_sentence(loaded[k])\n",
    "\n",
    "        if isinstance(corpus_token_list, dict):\n",
    "          corpus_token_list[k] = (token_list)\n",
    "        else:\n",
    "          corpus_token_list.append(token_list)\n",
    "\n",
    "        corpus_name.append(k)\n",
    "        corpus_dic[k] = i\n",
    "\n",
    "    return corpus_token_list, corpus_name, corpus_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_corpus(kept, loaded):\n",
    "    summariser = pipeline(\"summarization\", model=\"Falconsai/medical_summarization\")\n",
    "    summaries = []\n",
    "\n",
    "    for doc in kept:\n",
    "        summaries.append(summariser(loaded[doc], min_length=10, max_length = 50, do_sample=False))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to normalise both the BM25 and the cosine similarity scores to better concatenate them\n",
    "\n",
    "def normalise(score):\n",
    "    return (score - np.mean(score)) / np.std(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We extract the cleaned generated corpus (with the length specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(keep_relevant = False, start = 0, end = 3192):\n",
    "    doc, req, req_doc = loadNFCorpus()\n",
    "\n",
    "    # 1 - we shorten the initial dictionaries to only keep the number of documents given in the function argument\n",
    "\n",
    "    kept_doc, kept_req, kept_req_doc = shorten_dictionary(doc, req, req_doc, keep_relevant, start, end)\n",
    "\n",
    "    # 2 - we summarize the documents and the requests\n",
    "\n",
    "    #summary_doc = summarise_corpus(kept_doc, doc)\n",
    "    #summary_req = summarise_corpus(kept_req, req)\n",
    "\n",
    "    # 3 - we register all of the vocabulary/words that is present inside the documents (we omitted stop words)\n",
    "\n",
    "    vocab_doc = generate_vocab(kept_doc, doc)\n",
    "    vocab_req = generate_vocab(kept_req, req)\n",
    "    #vocab_sum = generate_vocab(kept_doc, summary_doc)\n",
    "\n",
    "    # 4 - we register the vocabulary that is present inside the corpus, the names of the documents inside the corpus and create a dictionary to enumerate them.\n",
    "\n",
    "    corpus_doc_token_list, corpus_doc_name, corpus_doc_dic = generate_corpus(kept_doc, doc, [])\n",
    "    corpus_req_token_dic, corpus_req_name, corpus_req_dic = generate_corpus(kept_req, req, {})\n",
    "\n",
    "    # 5 - we turn the elements from the request dictionary into a list\n",
    "\n",
    "    corpus_req_token_list = []\n",
    "    for v in corpus_req_token_dic.values():\n",
    "        corpus_req_token_list.append(v)\n",
    "\n",
    "    return corpus_doc_token_list, corpus_req_token_list, corpus_req_token_dic, corpus_doc_dic, kept_req_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create our model / load imported models and run scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load an external GloVe model\n",
    "\n",
    "external_model = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "temp_file = \"models/temp_glove_model.txt\"\n",
    "external_model.save_word2vec_format(temp_file, binary=False)\n",
    "\n",
    "# We load the saved model using KeyedVectors and convert it to word2vec\n",
    "glove_model = KeyedVectors.load_word2vec_format(temp_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load a BioWordVec model - it is tailored to capture semantic relationships and similarities among words in the context of biological and biomedical text.\n",
    "\n",
    "model_path = 'models/BioWordVec.vec.bin'\n",
    "\n",
    "# Load the external model using Gensim\n",
    "bio_model = KeyedVectors.load_word2vec_format(model_path, binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We run the tests to get results about the bm25 and word2vec scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_get_scores(reqTokenList, corpusDocTokenList, model):\n",
    "    scores = np.array([])\n",
    "\n",
    "    for doc_token in corpusDocTokenList:\n",
    "        # This is used to calculate the similarity between the request and the document\n",
    "        similarity = model.n_similarity(reqTokenList, doc_token)\n",
    "        scores = np.append(scores, similarity)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine the scores from the BM25 and the cosine similarity using a weighted average\n",
    "\n",
    "def combine_score(bm25_score, model_score, alpha):\n",
    "    return (alpha * bm25_score) + (1 - alpha) * ( (model_score['word2vec'][0] * model_score['word2vec'][1]) + (model_score['glove'][0] * model_score['glove'][1]) + (model_score['biovec'][0] * model_score['biovec'][1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(corpusDocTokenList, corpusReqTokenDic, corpusReqTokenList, corpusDicoDocName, dicReqDocToKeep, models, alpha_value = alpha_value):\n",
    "  bm25 = BM25Okapi(corpusDocTokenList)\n",
    "\n",
    "  ndcgCumul=0\n",
    "  corpusReqVec={}\n",
    "  ndcgBM25Cumul=0\n",
    "  nbReq=0\n",
    "  ndcgTop=3\n",
    "\n",
    "  for req in corpusReqTokenList:\n",
    "    reqTokenList = corpusReqTokenList[req]\n",
    "    \n",
    "    bm25_scores = normalise(bm25.get_scores(reqTokenList))\n",
    "    word2vec_scores = normalise(word2vec_get_scores(reqTokenList, corpusDocTokenList, models[1]))\n",
    "    glove_scores = normalise(word2vec_get_scores(reqTokenList, corpusDocTokenList, models[1]))\n",
    "    bio_scores = normalise(word2vec_get_scores(reqTokenList, corpusDocTokenList, models[1]))\n",
    "    \n",
    "    models_scores = {'word2vec' : [word2vec_scores, 0.1],\n",
    "                     'glove' : [glove_scores, 0.1],\n",
    "                     'biovec' : [bio_scores, 0.8]}\n",
    "    \n",
    "    combined_scores = combine_score(bm25_scores, models_scores, alpha_value)\n",
    "\n",
    "    trueDocs = np.zeros(len(corpusDocTokenList))\n",
    "\n",
    "    for docId in corpusDicoDocName:\n",
    "      if req in dicReqDocToKeep:\n",
    "        if docId in dicReqDocToKeep[req]:\n",
    "          posDocId = corpusDicoDocName[docId]\n",
    "          trueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
    "\n",
    "    ndcgBM25Cumul += ndcg_score([trueDocs], [combined_scores], k=ndcgTop)\n",
    "    nbReq += 1\n",
    "\n",
    "  ndcgBM25Cumul /= nbReq\n",
    "  return ndcgBM25Cumul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_score(w2v_vector_size = w2v_vector_size, w2v_window = w2v_window, w2v_min_count = w2v_min_count, w2v_sg = w2v_sg, alpha_value = alpha_value, keep_relevant_documents = keep_relevant_documents, start = start, end = end):\n",
    "    # We define the model chosen\n",
    "\n",
    "    corpus = extract_corpus(keep_relevant_documents, start, end)\n",
    "\n",
    "    # Define the models\n",
    "    models = {\n",
    "        1 : Word2Vec(sentences=corpus[0] + corpus[1], vector_size=w2v_vector_size, window=w2v_window, min_count=w2v_min_count, workers=8, sg=w2v_sg, seed = 42).wv,\n",
    "        2 : glove_model,\n",
    "        3 : bio_model\n",
    "    }\n",
    "\n",
    "    # chosen_model = 1\n",
    "    # model = models[chosen_model]\n",
    "\n",
    "    outputted_score = get_scores(*corpus, models)\n",
    "\n",
    "    print(f\"Final score = {outputted_score}\")\n",
    "\n",
    "    return outputted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score = 0.8163401592471555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8163401592471555"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_evaluation(factors):\n",
    "    # Initialize an empty list to store the scores\n",
    "    scores = []\n",
    "\n",
    "    for factor in factors:\n",
    "        factor_value = factor\n",
    "        score = output_score(w2v_window=factor_value)\n",
    "        scores.append(score)\n",
    "\n",
    "    plt.plot(scores, factor_value)\n",
    "    plt.xlabel('Minimum Word Length')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Effect of Minimum Word Length on Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score = 0.8163401592471555\n",
      "Final score = 0.757680318494311\n",
      "Final score = 0.7206700796235778\n",
      "Final score = 0.7360307166121994\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (4,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine Learning For NLP/Project/final_code.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m end \u001b[39m=\u001b[39m \u001b[39m150\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m chosen_model \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39m# 1 for my model, 2 for glove model, 3 for biovec model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m graph_evaluation(\u001b[39mrange\u001b[39;49m(\u001b[39m10\u001b[39;49m,\u001b[39m50\u001b[39;49m,\u001b[39m10\u001b[39;49m))\n",
      "\u001b[1;32m/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine Learning For NLP/Project/final_code.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     score \u001b[39m=\u001b[39m output_score(w2v_window\u001b[39m=\u001b[39mfactor_value)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     scores\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(scores, factor_value)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mMinimum Word Length\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexandrecogordan/Documents/ESILV/Ongoing/Machine%20Learning%20For%20NLP/Project/final_code.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/matplotlib/pyplot.py:3578\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   3571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\n\u001b[1;32m   3572\u001b[0m     \u001b[39m*\u001b[39margs: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m ArrayLike \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3577\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3578\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   3579\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m   3580\u001b[0m         scalex\u001b[39m=\u001b[39;49mscalex,\n\u001b[1;32m   3581\u001b[0m         scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   3582\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   3583\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3584\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (4,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcu0lEQVR4nO3db2yV5f348U9paaturRG0FkEEpxMl6mgDo6wandag0ZBskcVF1GliszmETqeMRYYxaXTRfXUKbgoaE3REReeDztEHG1Zxf2DFGCFxEWZBW0kxtqhbGXD/Hhj6W9fiOLV/uNrXK7kfnMv7Puc6uazn7X2fP3lZlmUBAJCAMcM9AQCAIyVcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGTkHC6vvPJKXHnllTFhwoTIy8uLF1988X8es2HDhqioqIji4uKYOnVqPProo/2ZKwAwyuUcLp988kmcd9558fDDDx/R/jt27IjLL788qquro7m5OX7yk5/EwoUL4/nnn895sgDA6Jb3RX5kMS8vL1544YWYN2/eYfe544474qWXXopt27Z1j9XW1sYbb7wRr7/+en8fGgAYhQoG+wFef/31qKmp6TF22WWXxapVq+Lf//53jB07ttcxXV1d0dXV1X374MGD8eGHH8a4ceMiLy9vsKcMAAyALMti7969MWHChBgzZmDeVjvo4dLW1hZlZWU9xsrKymL//v3R3t4e5eXlvY6pr6+P5cuXD/bUAIAhsHPnzpg4ceKA3Negh0tE9DpLcujq1OHOnixZsiTq6uq6b3d0dMSpp54aO3fujJKSksGbKAAwYDo7O2PSpEnx5S9/ecDuc9DD5eSTT462trYeY7t3746CgoIYN25cn8cUFRVFUVFRr/GSkhLhAgCJGci3eQz697jMnj07Ghsbe4ytX78+Kisr+3x/CwDA4eQcLh9//HFs2bIltmzZEhGffdx5y5Yt0dLSEhGfXeZZsGBB9/61tbXx7rvvRl1dXWzbti1Wr14dq1atittuu21gngEAMGrkfKlo06ZNcdFFF3XfPvRelOuuuy6efPLJaG1t7Y6YiIgpU6ZEQ0NDLF68OB555JGYMGFCPPTQQ/Gtb31rAKYPAIwmX+h7XIZKZ2dnlJaWRkdHh/e4AEAiBuP1228VAQDJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjH6Fy4oVK2LKlClRXFwcFRUV0dTU9Ln7r1mzJs4777w49thjo7y8PG644YbYs2dPvyYMAIxeOYfL2rVrY9GiRbF06dJobm6O6urqmDt3brS0tPS5/6uvvhoLFiyIG2+8Md5666149tln469//WvcdNNNX3jyAMDoknO4PPDAA3HjjTfGTTfdFNOmTYv/+7//i0mTJsXKlSv73P9Pf/pTnHbaabFw4cKYMmVKfOMb34ibb745Nm3a9IUnDwCMLjmFy759+2Lz5s1RU1PTY7ympiY2btzY5zFVVVWxa9euaGhoiCzL4oMPPojnnnsurrjiisM+TldXV3R2dvbYAAByCpf29vY4cOBAlJWV9RgvKyuLtra2Po+pqqqKNWvWxPz586OwsDBOPvnkOP744+OXv/zlYR+nvr4+SktLu7dJkyblMk0AYITq15tz8/LyetzOsqzX2CFbt26NhQsXxl133RWbN2+Ol19+OXbs2BG1tbWHvf8lS5ZER0dH97Zz587+TBMAGGEKctl5/PjxkZ+f3+vsyu7du3udhTmkvr4+5syZE7fffntERJx77rlx3HHHRXV1ddxzzz1RXl7e65iioqIoKirKZWoAwCiQ0xmXwsLCqKioiMbGxh7jjY2NUVVV1ecxn376aYwZ0/Nh8vPzI+KzMzUAAEcq50tFdXV18fjjj8fq1atj27ZtsXjx4mhpaem+9LNkyZJYsGBB9/5XXnllrFu3LlauXBnbt2+P1157LRYuXBgzZ86MCRMmDNwzAQBGvJwuFUVEzJ8/P/bs2RN33313tLa2xvTp06OhoSEmT54cERGtra09vtPl+uuvj71798bDDz8cP/rRj+L444+Piy++OO69996BexYAwKiQlyVwvaazszNKS0ujo6MjSkpKhns6AMARGIzXb79VBAAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMvoVLitWrIgpU6ZEcXFxVFRURFNT0+fu39XVFUuXLo3JkydHUVFRnH766bF69ep+TRgAGL0Kcj1g7dq1sWjRolixYkXMmTMnfvWrX8XcuXNj69atceqpp/Z5zNVXXx0ffPBBrFq1Kr7yla/E7t27Y//+/V948gDA6JKXZVmWywGzZs2KGTNmxMqVK7vHpk2bFvPmzYv6+vpe+7/88svxne98J7Zv3x4nnHBCvybZ2dkZpaWl0dHRESUlJf26DwBgaA3G63dOl4r27dsXmzdvjpqamh7jNTU1sXHjxj6Peemll6KysjLuu+++OOWUU+LMM8+M2267Lf75z38e9nG6urqis7OzxwYAkNOlovb29jhw4ECUlZX1GC8rK4u2trY+j9m+fXu8+uqrUVxcHC+88EK0t7fH97///fjwww8P+z6X+vr6WL58eS5TAwBGgX69OTcvL6/H7SzLeo0dcvDgwcjLy4s1a9bEzJkz4/LLL48HHnggnnzyycOedVmyZEl0dHR0bzt37uzPNAGAESanMy7jx4+P/Pz8XmdXdu/e3esszCHl5eVxyimnRGlpaffYtGnTIsuy2LVrV5xxxhm9jikqKoqioqJcpgYAjAI5nXEpLCyMioqKaGxs7DHe2NgYVVVVfR4zZ86ceP/99+Pjjz/uHnv77bdjzJgxMXHixH5MGQAYrXK+VFRXVxePP/54rF69OrZt2xaLFy+OlpaWqK2tjYjPLvMsWLCge/9rrrkmxo0bFzfccENs3bo1Xnnllbj99tvje9/7XhxzzDED90wAgBEv5+9xmT9/fuzZsyfuvvvuaG1tjenTp0dDQ0NMnjw5IiJaW1ujpaWle/8vfelL0djYGD/84Q+jsrIyxo0bF1dffXXcc889A/csAIBRIefvcRkOvscFANIz7N/jAgAwnIQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJKNf4bJixYqYMmVKFBcXR0VFRTQ1NR3Rca+99loUFBTE+eef35+HBQBGuZzDZe3atbFo0aJYunRpNDc3R3V1dcydOzdaWlo+97iOjo5YsGBBfPOb3+z3ZAGA0S0vy7IslwNmzZoVM2bMiJUrV3aPTZs2LebNmxf19fWHPe473/lOnHHGGZGfnx8vvvhibNmy5bD7dnV1RVdXV/ftzs7OmDRpUnR0dERJSUku0wUAhklnZ2eUlpYO6Ot3Tmdc9u3bF5s3b46ampoe4zU1NbFx48bDHvfEE0/EO++8E8uWLTuix6mvr4/S0tLubdKkSblMEwAYoXIKl/b29jhw4ECUlZX1GC8rK4u2trY+j/n73/8ed955Z6xZsyYKCgqO6HGWLFkSHR0d3dvOnTtzmSYAMEIdWUn8l7y8vB63syzrNRYRceDAgbjmmmti+fLlceaZZx7x/RcVFUVRUVF/pgYAjGA5hcv48eMjPz+/19mV3bt39zoLExGxd+/e2LRpUzQ3N8ctt9wSEREHDx6MLMuioKAg1q9fHxdffPEXmD4AMJrkdKmosLAwKioqorGxscd4Y2NjVFVV9dq/pKQk3nzzzdiyZUv3VltbG1/96ldjy5YtMWvWrC82ewBgVMn5UlFdXV1ce+21UVlZGbNnz45f//rX0dLSErW1tRHx2ftT3nvvvXjqqadizJgxMX369B7Hn3TSSVFcXNxrHADgf8k5XObPnx979uyJu+++O1pbW2P69OnR0NAQkydPjoiI1tbW//mdLgAA/ZHz97gMh8H4HDgAMLiG/XtcAACGk3ABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZPQrXFasWBFTpkyJ4uLiqKioiKampsPuu27durj00kvjxBNPjJKSkpg9e3b8/ve/7/eEAYDRK+dwWbt2bSxatCiWLl0azc3NUV1dHXPnzo2WlpY+93/llVfi0ksvjYaGhti8eXNcdNFFceWVV0Zzc/MXnjwAMLrkZVmW5XLArFmzYsaMGbFy5crusWnTpsW8efOivr7+iO7jnHPOifnz58ddd93V5z/v6uqKrq6u7tudnZ0xadKk6OjoiJKSklymCwAMk87OzigtLR3Q1++czrjs27cvNm/eHDU1NT3Ga2pqYuPGjUd0HwcPHoy9e/fGCSeccNh96uvro7S0tHubNGlSLtMEAEaonMKlvb09Dhw4EGVlZT3Gy8rKoq2t7Yju4/77749PPvkkrr766sPus2TJkujo6Ojedu7cmcs0AYARqqA/B+Xl5fW4nWVZr7G+PPPMM/Gzn/0sfvvb38ZJJ5102P2KioqiqKioP1MDAEawnMJl/PjxkZ+f3+vsyu7du3udhflva9eujRtvvDGeffbZuOSSS3KfKQAw6uV0qaiwsDAqKiqisbGxx3hjY2NUVVUd9rhnnnkmrr/++nj66afjiiuu6N9MAYBRL+dLRXV1dXHttddGZWVlzJ49O379619HS0tL1NbWRsRn709577334qmnnoqIz6JlwYIF8eCDD8bXv/717rM1xxxzTJSWlg7gUwEARrqcw2X+/PmxZ8+euPvuu6O1tTWmT58eDQ0NMXny5IiIaG1t7fGdLr/61a9i//798YMf/CB+8IMfdI9fd9118eSTT37xZwAAjBo5f4/LcBiMz4EDAINr2L/HBQBgOAkXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASEa/wmXFihUxZcqUKC4ujoqKimhqavrc/Tds2BAVFRVRXFwcU6dOjUcffbRfkwUARrecw2Xt2rWxaNGiWLp0aTQ3N0d1dXXMnTs3Wlpa+tx/x44dcfnll0d1dXU0NzfHT37yk1i4cGE8//zzX3jyAMDokpdlWZbLAbNmzYoZM2bEypUru8emTZsW8+bNi/r6+l7733HHHfHSSy/Ftm3busdqa2vjjTfeiNdff73Px+jq6oqurq7u2x0dHXHqqafGzp07o6SkJJfpAgDDpLOzMyZNmhQfffRRlJaWDsydZjno6urK8vPzs3Xr1vUYX7hwYXbBBRf0eUx1dXW2cOHCHmPr1q3LCgoKsn379vV5zLJly7KIsNlsNpvNNgK2d955J5fc+FwFkYP29vY4cOBAlJWV9RgvKyuLtra2Po9pa2vrc//9+/dHe3t7lJeX9zpmyZIlUVdX1337o48+ismTJ0dLS8vAFRv9cqienf0aftbi6GEtji7W4+hx6IrJCSecMGD3mVO4HJKXl9fjdpZlvcb+1/59jR9SVFQURUVFvcZLS0v9S3iUKCkpsRZHCWtx9LAWRxfrcfQYM2bgPsSc0z2NHz8+8vPze51d2b17d6+zKoecfPLJfe5fUFAQ48aNy3G6AMBollO4FBYWRkVFRTQ2NvYYb2xsjKqqqj6PmT17dq/9169fH5WVlTF27NgcpwsAjGY5n7upq6uLxx9/PFavXh3btm2LxYsXR0tLS9TW1kbEZ+9PWbBgQff+tbW18e6770ZdXV1s27YtVq9eHatWrYrbbrvtiB+zqKgoli1b1uflI4aWtTh6WIujh7U4uliPo8dgrEXOH4eO+OwL6O67775obW2N6dOnxy9+8Yu44IILIiLi+uuvj3/84x/xxz/+sXv/DRs2xOLFi+Ott96KCRMmxB133NEdOgAAR6pf4QIAMBz8VhEAkAzhAgAkQ7gAAMkQLgBAMo6acFmxYkVMmTIliouLo6KiIpqamj53/w0bNkRFRUUUFxfH1KlT49FHHx2imY58uazFunXr4tJLL40TTzwxSkpKYvbs2fH73/9+CGc7suX6d3HIa6+9FgUFBXH++ecP7gRHkVzXoqurK5YuXRqTJ0+OoqKiOP3002P16tVDNNuRLde1WLNmTZx33nlx7LHHRnl5edxwww2xZ8+eIZrtyPXKK6/ElVdeGRMmTIi8vLx48cUX/+cxA/LaPWC/evQF/OY3v8nGjh2bPfbYY9nWrVuzW2+9NTvuuOOyd999t8/9t2/fnh177LHZrbfemm3dujV77LHHsrFjx2bPPffcEM985Ml1LW699dbs3nvvzf7yl79kb7/9drZkyZJs7Nix2d/+9rchnvnIk+taHPLRRx9lU6dOzWpqarLzzjtvaCY7wvVnLa666qps1qxZWWNjY7Zjx47sz3/+c/baa68N4axHplzXoqmpKRszZkz24IMPZtu3b8+ampqyc845J5s3b94Qz3zkaWhoyJYuXZo9//zzWURkL7zwwufuP1Cv3UdFuMycOTOrra3tMXbWWWdld955Z5/7//jHP87OOuusHmM333xz9vWvf33Q5jha5LoWfTn77LOz5cuXD/TURp3+rsX8+fOzn/70p9myZcuEywDJdS1+97vfZaWlpdmePXuGYnqjSq5r8fOf/zybOnVqj7GHHnoomzhx4qDNcTQ6knAZqNfuYb9UtG/fvti8eXPU1NT0GK+pqYmNGzf2eczrr7/ea//LLrssNm3aFP/+978Hba4jXX/W4r8dPHgw9u7dO6C/BDoa9XctnnjiiXjnnXdi2bJlgz3FUaM/a/HSSy9FZWVl3HfffXHKKafEmWeeGbfddlv885//HIopj1j9WYuqqqrYtWtXNDQ0RJZl8cEHH8Rzzz0XV1xxxVBMmf8wUK/d/fp16IHU3t4eBw4c6PUjjWVlZb1+nPGQtra2Pvffv39/tLe3R3l5+aDNdyTrz1r8t/vvvz8++eSTuPrqqwdjiqNGf9bi73//e9x5553R1NQUBQXD/qc9YvRnLbZv3x6vvvpqFBcXxwsvvBDt7e3x/e9/Pz788EPvc/kC+rMWVVVVsWbNmpg/f37861//iv3798dVV10Vv/zlL4diyvyHgXrtHvYzLofk5eX1uJ1lWa+x/7V/X+PkLte1OOSZZ56Jn/3sZ7F27do46aSTBmt6o8qRrsWBAwfimmuuieXLl8eZZ545VNMbVXL5uzh48GDk5eXFmjVrYubMmXH55ZfHAw88EE8++aSzLgMgl7XYunVrLFy4MO66667YvHlzvPzyy7Fjxw4/OzNMBuK1e9j/t2z8+PGRn5/fq5Z3797dq8wOOfnkk/vcv6CgIMaNGzdocx3p+rMWh6xduzZuvPHGePbZZ+OSSy4ZzGmOCrmuxd69e2PTpk3R3Nwct9xyS0R89uKZZVkUFBTE+vXr4+KLLx6SuY80/fm7KC8vj1NOOSVKS0u7x6ZNmxZZlsWuXbvijDPOGNQ5j1T9WYv6+vqYM2dO3H777RERce6558Zxxx0X1dXVcc899zhDP4QG6rV72M+4FBYWRkVFRTQ2NvYYb2xsjKqqqj6PmT17dq/9169fH5WVlTF27NhBm+tI15+1iPjsTMv1118fTz/9tOvGAyTXtSgpKYk333wztmzZ0r3V1tbGV7/61diyZUvMmjVrqKY+4vTn72LOnDnx/vvvx8cff9w99vbbb8eYMWNi4sSJgzrfkaw/a/Hpp5/GmDE9X+ry8/Mj4v//3z5DY8Beu3N6K+8gOfTxtlWrVmVbt27NFi1alB133HHZP/7xjyzLsuzOO+/Mrr322u79D32kavHixdnWrVuzVatW+Tj0AMl1LZ5++umsoKAge+SRR7LW1tbu7aOPPhqupzBi5LoW/82nigZOrmuxd+/ebOLEidm3v/3t7K233so2bNiQnXHGGdlNN900XE9hxMh1LZ544omsoKAgW7FiRfbOO+9kr776alZZWZnNnDlzuJ7CiLF3796subk5a25uziIie+CBB7Lm5ubuj6YP1mv3UREuWZZljzzySDZ58uSssLAwmzFjRrZhw4buf3bddddlF154YY/9//jHP2Zf+9rXssLCwuy0007LVq5cOcQzHrlyWYsLL7wwi4he23XXXTf0Ex+Bcv27+E/CZWDluhbbtm3LLrnkkuyYY47JJk6cmNXV1WWffvrpEM96ZMp1LR566KHs7LPPzo455pisvLw8++53v5vt2rVriGc98vzhD3/43P/+D9Zrd16WOVcGAKRh2N/jAgBwpIQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAk4/8BrQWhjBP+6s8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep_relevant_documents = False\n",
    "w2v_vector_size = 200\n",
    "w2v_window = 10\n",
    "w2v_min_count = 1\n",
    "w2v_sg = 1\n",
    "alpha_value = 0.5\n",
    "start = 0\n",
    "end = 150\n",
    "\n",
    "graph_evaluation(range(10,50,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pattern', 0.9893303513526917),\n",
       " ('fried', 0.9875578284263611),\n",
       " ('cohort', 0.9857393503189087),\n",
       " ('epidemiologic', 0.9837522506713867),\n",
       " ('french', 0.9831963777542114)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['food', 'western', 'risk'], negative=['healthy'], topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
