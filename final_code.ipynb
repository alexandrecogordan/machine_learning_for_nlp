{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "## By BROSSEAU Alexandre & COGORDAN Alexandre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the necessary module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import ndcg_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexandrecogordan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexandrecogordan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the NFCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNFCorpus():\n",
    "\tdir = \"./project1-2023/\"\n",
    "\tfilename = dir +\"dev.docs\"\n",
    "\n",
    "\tdicDoc={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\tdicDoc[key] = value\n",
    "\tfilename = dir + \"dev.all.queries\"\n",
    "\tdicReq={}\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\tdicReq[key] = value\n",
    "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
    "\tdicReqDoc=defaultdict(dict)\n",
    "\twith open(filename) as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.strip().split('\\t')\n",
    "\t\treq = tabLine[0]\n",
    "\t\tdoc = tabLine[2]\n",
    "\t\tscore = int(tabLine[3])\n",
    "\t\tdicReqDoc[req][doc]=score\n",
    "\n",
    "\treturn dicDoc, dicReq, dicReqDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where we'll change our main parameters to test and evaluate the different scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_word_length = 3\n",
    "keep_relevant_documents = False\n",
    "start = 0\n",
    "end = 150\n",
    "w2v_vector_size = 200\n",
    "w2v_window = 10\n",
    "w2v_min_count = 1\n",
    "w2v_sg = 1\n",
    "alpha_value = 0.5\n",
    "chosen_model = 3 # 1 for my model, 2 for glove model, 3 for biovec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a shortened corpus where we only keep the documents that are indicated by start and end\n",
    "# Keep relevant: only keeps the relevant documents to their respective queries\n",
    "\n",
    "def shorten_dictionary(doc, req, req_doc, keep_relevant, start, end):\n",
    "  kept_doc = []\n",
    "  kept_req = []\n",
    "  kept_req_doc = defaultdict(dict)\n",
    "\n",
    "  if(keep_relevant):\n",
    "    req_doc = {outer_key: {inner_key: inner_value for inner_key, inner_value in inner_dict.items() if inner_value == 2}\n",
    "                            for outer_key, inner_dict in req_doc.items()}\n",
    "\n",
    "  count = start\n",
    "  for req in req_doc:\n",
    "    if count > (end - start):\n",
    "      break\n",
    "    for doc in req_doc[req]:\n",
    "      kept_req_doc[req][doc] = req_doc[req][doc]\n",
    "      kept_doc.append(doc)\n",
    "      count += 1\n",
    "    kept_req.append(req)\n",
    "  kept_doc = list(set(kept_doc)) # We remove duplicates\n",
    "\n",
    "  return kept_doc, kept_req, kept_req_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence, minimum_word_length = minimum_word_length):\n",
    "\n",
    "    # Remove stopwords\n",
    "\tstopword = stopwords.words('english')\n",
    "\n",
    "\t# Remove URLs\n",
    "\tsentence = re.sub(r'http\\S+', '', sentence, flags=re.MULTILINE)\n",
    "\tsentence = re.sub(r'www\\S+', '', sentence, flags=re.MULTILINE)\n",
    "\n",
    "\t# Remove numbers\n",
    "\tsentence = re.sub(r'\\d', '', sentence)\n",
    "\n",
    "\t# Remove punctuation except for certain cases (hyphens, apostrophes, etc.)\n",
    "\tsentence = re.sub(r'[^\\w\\s\\-/]', '', sentence)\n",
    "\n",
    "    # We tokenize the sentence\n",
    "\tword_tokens = word_tokenize(sentence.lower())\t\t\n",
    "\t\n",
    "    # We lemmatize the tokens - lemmatization is the process of converting a word to its base form\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tword_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]\n",
    "\n",
    "    # We remove stopwords and words with less than 3 characters\n",
    "\tword_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word) > minimum_word_length]\n",
    "\treturn word_tokens_without_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(kept, loaded, summary = False):\n",
    "\n",
    "  vocab = set()\n",
    "\n",
    "  for k in kept:\n",
    "    if(summary != False):\n",
    "      vocab.update(clean_sentence(loaded[k]))\n",
    "    else:\n",
    "      vocab.update(clean_sentence(k))\n",
    "\n",
    "  return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(kept, loaded, corpus_token_list):\n",
    "    corpus_name = []\n",
    "    corpus_dic = {}\n",
    "\n",
    "    for i, k in enumerate(kept):\n",
    "        token_list = clean_sentence(loaded[k])\n",
    "\n",
    "        if isinstance(corpus_token_list, dict):\n",
    "          corpus_token_list[k] = (token_list)\n",
    "        else:\n",
    "          corpus_token_list.append(token_list)\n",
    "\n",
    "        corpus_name.append(k)\n",
    "        corpus_dic[k] = i\n",
    "\n",
    "    return corpus_token_list, corpus_name, corpus_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_corpus(kept, loaded):\n",
    "    summariser = pipeline(\"summarization\", model=\"Falconsai/medical_summarization\")\n",
    "    summaries = []\n",
    "\n",
    "    for doc in kept:\n",
    "        summaries.append(summariser(loaded[doc], min_length=10, max_length = 50, do_sample=False))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to normalise both the BM25 and the cosine similarity scores to better concatenate them\n",
    "\n",
    "def normalise(score):\n",
    "    return (score - np.mean(score)) / np.std(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We extract the cleaned generated corpus (with the length specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(keep_relevant = False, start = 0, end = 3192):\n",
    "    doc, req, req_doc = loadNFCorpus()\n",
    "\n",
    "    # 1 - we shorten the initial dictionaries to only keep the number of documents given in the function argument\n",
    "\n",
    "    kept_doc, kept_req, kept_req_doc = shorten_dictionary(doc, req, req_doc, keep_relevant, start, end)\n",
    "\n",
    "    # 2 - we summarize the documents and the requests\n",
    "\n",
    "    #summary_doc = summarise_corpus(kept_doc, doc)\n",
    "    #summary_req = summarise_corpus(kept_req, req)\n",
    "\n",
    "    # 3 - we register all of the vocabulary/words that is present inside the documents (we omitted stop words)\n",
    "\n",
    "    vocab_doc = generate_vocab(kept_doc, doc)\n",
    "    vocab_req = generate_vocab(kept_req, req)\n",
    "    #vocab_sum = generate_vocab(kept_doc, summary_doc)\n",
    "\n",
    "    # 4 - we register the vocabulary that is present inside the corpus, the names of the documents inside the corpus and create a dictionary to enumerate them.\n",
    "\n",
    "    corpus_doc_token_list, corpus_doc_name, corpus_doc_dic = generate_corpus(kept_doc, doc, [])\n",
    "    corpus_req_token_dic, corpus_req_name, corpus_req_dic = generate_corpus(kept_req, req, {})\n",
    "\n",
    "    # 5 - we turn the elements from the request dictionary into a list\n",
    "\n",
    "    corpus_req_token_list = []\n",
    "    for v in corpus_req_token_dic.values():\n",
    "        corpus_req_token_list.append(v)\n",
    "\n",
    "    return corpus_doc_token_list, corpus_req_token_list, corpus_req_token_dic, corpus_doc_dic, kept_req_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We run the tests to get results about the bm25 and word2vec scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_get_scores(reqTokenList, corpusDocTokenList):\n",
    "    scores = np.array([])\n",
    "\n",
    "    for doc_token in corpusDocTokenList:\n",
    "        # This is used to calculate the similarity between the request and the document\n",
    "        similarity = model.n_similarity(reqTokenList, doc_token)\n",
    "        scores = np.append(scores, similarity)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine the scores from the BM25 and the cosine similarity using a weighted average\n",
    "\n",
    "def combine_score(bm25_score, model_score, alpha = alpha_value):\n",
    "    return (alpha * bm25_score) + (1 - alpha) * model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(corpusDocTokenList, corpusReqTokenDic, corpusReqTokenList, corpusDicoDocName, dicReqDocToKeep):\n",
    "  bm25 = BM25Okapi(corpusDocTokenList)\n",
    "\n",
    "  ndcgCumul=0\n",
    "  corpusReqVec={}\n",
    "  ndcgBM25Cumul=0\n",
    "  nbReq=0\n",
    "  ndcgTop=3\n",
    "\n",
    "  for req in corpusReqTokenList:\n",
    "    reqTokenList = corpusReqTokenList[req]\n",
    "    \n",
    "    bm25_scores = normalise(bm25.get_scores(reqTokenList))\n",
    "    word2vec_scores = normalise(word2vec_get_scores(reqTokenList, corpusDocTokenList))\n",
    "    combined_scores = combine_score(bm25_scores, word2vec_scores)\n",
    "\n",
    "    trueDocs = np.zeros(len(corpusDocTokenList))\n",
    "\n",
    "    for docId in corpusDicoDocName:\n",
    "      if req in dicReqDocToKeep:\n",
    "        if docId in dicReqDocToKeep[req]:\n",
    "          posDocId = corpusDicoDocName[docId]\n",
    "          trueDocs[posDocId] = dicReqDocToKeep[req][docId]\n",
    "\n",
    "    ndcgBM25Cumul += ndcg_score([trueDocs], [combined_scores], k=ndcgTop)\n",
    "    nbReq += 1\n",
    "\n",
    "  ndcgBM25Cumul /= nbReq\n",
    "  return ndcgBM25Cumul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We create our model / load imported models and run scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load an external GloVe model\n",
    "\n",
    "external_model = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "temp_file = \"models/temp_glove_model.txt\"\n",
    "external_model.save_word2vec_format(temp_file, binary=False)\n",
    "\n",
    "# We load the saved model using KeyedVectors and convert it to word2vec\n",
    "glove_model = KeyedVectors.load_word2vec_format(temp_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load a BioWordVec model - it is tailored to capture semantic relationships and similarities among words in the context of biological and biomedical text.\n",
    "\n",
    "model_path = 'models/BioWordVec.vec.bin'\n",
    "\n",
    "# Load the external model using Gensim\n",
    "bio_model = KeyedVectors.load_word2vec_format(model_path, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score = 0.8163401592471555\n"
     ]
    }
   ],
   "source": [
    "# We define the model chosen\n",
    "\n",
    "corpus = extract_corpus(keep_relevant_documents, start, end)\n",
    "\n",
    "chosen_model = 1\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    1 : Word2Vec(sentences=corpus[0] + corpus[1], vector_size=w2v_vector_size, window=w2v_window, min_count=w2v_min_count, workers=8, sg=w2v_sg, seed = 42).wv,\n",
    "    2 : glove_model,\n",
    "    3 : bio_model\n",
    "}\n",
    "\n",
    "model = models[chosen_model]\n",
    "\n",
    "print(f\"Final score = {get_scores(*corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_word_length = 3\n",
    "keep_relevant_documents = False\n",
    "start = 0\n",
    "end = 150\n",
    "w2v_vector_size = 200\n",
    "w2v_window = 10\n",
    "w2v_min_count = 1\n",
    "w2v_sg = 1\n",
    "alpha_value = 0.5\n",
    "chosen_model = 3 # 1 for my model, 2 for glove model, 3 for biovec model\n",
    "\n",
    "for :\n",
    "    print(i, model[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pattern', 0.9893303513526917),\n",
       " ('fried', 0.9875578284263611),\n",
       " ('cohort', 0.9857393503189087),\n",
       " ('epidemiologic', 0.9837522506713867),\n",
       " ('french', 0.9831963777542114)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['food', 'western', 'risk'], negative=['healthy'], topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
